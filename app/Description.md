### Общее описание:

Фидж - голосовой ассистент. Есть база ссылок на статьи, которые человек прочитал и считает интересными. Фидж необходим, чтобы эффективно ссылаться на источники во время беседы/презентации. На основании голосового диалога производится поиск по базе и предлагается наиболее релевантный вариант.

На данном этапе Фидж работает с заранее прописанными текстовыми запросами, которые хранятся в queries.json или с текстовыми запросами поступающими от пользователя через приложение.
Для поиска используются модели тематического поиска: lda, lsi, fasttext и word2vec.
После очереди тестов, наиболее успешной была выбрана и взята за основную модель lsi. Однако, функционал остальных моделей был оставлен, так что при необходимости есть возможность переключиться на них.
Для работы моделей производится преобразование статей и запросов в нужный вид.

Пользователь выбирает модель, название модели передается для поиска (по умолчанию lsi). Если модель ещё не была обучена и, соответственно, сохранена, происходит обучение модели и ее сохранение на диск. В ином случае, происходит считывание модели и запросов, обработка запросов и получение результата работы моделей для каждого запроса.

### Реализованный функционал:

Считывание ссылок для загрузки статей из файла urls.json.
***
Скачивание html страниц по ссылкам.
***
Выделение текста со страниц.
***
Чистка текста от пунктуации, табуляции, стопслов, приведение текста к нижнему регистру, лемматизация.
***
Сохранение текстов статей в виде списка нормализованных слов.
***
Для английских статей производится перевод текста. Определение языка статьи реализовано двумя способами:
1) Язык заранее указан в файле urls.json;
2) Использованы библиотеки для определения языка.
***
В случае добавления новых ссылок в urls.json производится дозакачка по новым ссылкам, которая подразумевает этапы:
- фильтрация новых ссылок;
- скачивание статей по новым ссылкам;
- предобработка текста;
- добавление новых статей к старым;
- сохранение.
***
В случае неудачной загрузки статьи по ссылке (не получена страница/не удалось выделить текст) ссылка игнорируется и при последующем запуске программы будет попытка повторной загрузки.
Причина неудачной загрузки выводится в консоль.
***
Сохранение обработанных статей в структуру PhyArticle и на диск в articles.json, содержащие поля:
- url - ссылка по которой загружена статья;
- title - название статьи;
- text - текст статьи;
- language - язык статьи;
- normalized_words - список нормализованных слов.
***
Составление корпуса и словаря для моделей из normalized_words (методами gensim).
***
Запросы хранятся в queries.json с полями:

- id - идентификационный номер запроса;
- url - ссылка статьи по которой сформирован запрос;
- title - название статьи;
- text - текст запроса (подразумевается копипаст);
- summary - текст запроса (подразумевается краткая выжимка/пересказ человека).

Тексты запросов также подвергаются предобработке, как и тексты статей.
***
Поиск статьи по запросу (необходимо передать корпус и подготовленный запрос; также в качестве аргумента можно указать число наиболее релевантных статей для вывода. параметр amount)
***

Запись результатов поиска в answers.json с информацией:

- answer_time - время поиска статьи;
- model_name - название модели;
- answer_articles - информация по найденной статье;
- query - текст запроса.

В свою очередь answer_articles содержит:

- id - идентификационный номер статьи;
- title - заголовок статьи;
- url - ссылка на статью;
- text - первые строки статьи;
- similarity - мера сходства с запросом.

***
Также реализован фунционал работы:   
- со статьями представленными в формате pdf (в репозитории имеется скрипт, который парсит pdf файлы и представляет их в необходимом виде нашему сервису).    
- с книгами издательства МИФ. Написан скраппер, который собирает описание книг с официального сайта издательства и представляет данные в необходимом для Phyge виде.  
  - tema - хештеги книги
  - title - название
  - source - ссылка
  - description - краткое описание
  - stikers - текст на стикерах
  - about_book - о книге
  - excerption - цитаты из книги
  - type = 'book'- тип данных  
Можно доскраппить данные:   
  - help_book - кому может помочь эта книга
  - for_who - для кого книга
  - about_author - об авторе
***
При первом запуске (./train.sh) происходит инициализация:
- Заполнение базы данных
- Обучение моделей

Заполняем базу данных ссылками на статьи/статьями в pdf/ подготовленными даннами в json (для книг). Есть возможность комбинировать.

Статьи помечаются полем "type": "article"/"pdf"/"book". В дальнейшем мы можем фильтровать то, с чем мы работаем из базы данных.
По умолчанию из БД достаются все статьи методом DBController.get_all_articles().

В классе DBController также уже реализованны фильтры для отдельного импорта articles/pdf/book.

Все настройки моделей находятся в классе TematicModels. Основной параметр для моделей lsi, lda это колличество тем TOPIC_NUMBER.
Опытным путем было установлено, что примерное кол-во тем колеблится вокруг 300 ±100 для 2-3к статей.

Таким образом на первом этапе необхоидмо определить:
- что будет находиться в БД
- обучать модель на всех данных или части
- примерное колличество тем - TOPIC_NUMBER

На выходе имеем:
- Заполненую базу данных (MongoBD)
- Файлы модели (./out/model_name) + корпус, словарь
- Сохраненные id статей, на которых обучалась модель (./out/model_name/articles_id.json)
- Сериализованный класс PhyBase
***
При втором запуске (./run.sh) модели загружаются в память и запускается сервер.
Каждый запрос от пользователя сохраняется в (./tmp/history_request.txt)

***
visualisationLda.ipynb виузализирует распределение слов по темам. Может помочь оценить колличество тем для моделей (TOPIC_NUMBER)
***
TagMining.ipynb может выделить несколько главных предложений в статье/описании книги и вывести ключевые слова.
